{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline  \n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import operator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('nosleep.csv')\n",
    "\n",
    "# Some posts were deleted\n",
    "df = df[df.Author != '[deleted]']\n",
    "\n",
    "# Some text is empty\n",
    "df = df[df.Text.isna() == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Word2Vec\n",
    "### 1.1 Preprocess Text and Prepare Word Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "NUM_STORIES = 2500\n",
    "\n",
    "# Take the first NUM_STORIES stories\n",
    "stories = df.Text[:NUM_STORIES]\n",
    "\n",
    "# Our final list of clean sentences\n",
    "data = []\n",
    "\n",
    "# Gather some statistics\n",
    "num_sentences = []\n",
    "num_unique_words = []\n",
    "\n",
    "# Our dictionary, pre-initialized with special tokens\n",
    "unique_words = set(['<PAD>','<UNK>, <NUM>'])\n",
    "\n",
    "# Word counts, so we can use the most common words\n",
    "word_counts = {}\n",
    "\n",
    "stories_handled = 0\n",
    "for this_story in stories:\n",
    "    \n",
    "    # A story is broken up into markdown paragraphs\n",
    "    paragraphs = list(filter(lambda x: len(x) > 0, this_story.split(\"\\n\")))\n",
    "    for this_paragraph in paragraphs:\n",
    "        \n",
    "        # If the paragram markdown contains a link, remove it\n",
    "        if \"[\" in this_paragraph and 'https://' in this_paragraph:\n",
    "            continue\n",
    "        \n",
    "        # Split paragraph into sentences based on end-of-sentence punctuation\n",
    "        all_sentences = re.split(\"[.!?]\", this_paragraph)\n",
    "        for this_sentence in all_sentences:\n",
    "            \n",
    "            # Remove punctuation symbols\n",
    "            for char in string.punctuation + \"’”“…—-\":\n",
    "                this_sentence = this_sentence.replace(char, ' ')\n",
    "                \n",
    "            # Remove multiple whitespace\n",
    "            this_sentence = re.sub(\"\\s{2,}\", \" \", this_sentence)\n",
    "                \n",
    "            # Remove initial and trailing whitespace, and make everything lowercase\n",
    "            this_sentence = this_sentence.strip().lower()\n",
    "            \n",
    "            # Replace numbers with a special <NUM> token\n",
    "            this_sentence = re.sub(\"\\d+\", \"<NUM>\", this_sentence)\n",
    "            \n",
    "            # Process non-empty sentences\n",
    "            if len(this_sentence) > 0:\n",
    "                data.append(this_sentence)\n",
    "                for this_word in this_sentence.split(' '):\n",
    "                    # Gather statistics\n",
    "                    unique_words.add(this_word)\n",
    "                    word_counts[this_word] = word_counts.get(this_word, 0) + 1\n",
    "        \n",
    "    stories_handled += 1\n",
    "    \n",
    "    # Print progress every once in a while\n",
    "    if stories_handled % 50 == 0:\n",
    "        print(\"Parsed {} stories\".format(stories_handled))\n",
    "        \n",
    "    # Append current statistics to a list \n",
    "    num_sentences.append(len(data))\n",
    "    num_unique_words.append(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample some sentences so we can have a look\n",
    "\n",
    "print(np.random.choice(data, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(20,7))\n",
    "plot(range(NUM_STORIES), num_sentences)\n",
    "xlabel(\"# Of Stories\")\n",
    "ylabel(\"# Of Sentences\");\n",
    "title(\"# of Stories vs. # of Sentences\")\n",
    "grid(axis='both')\n",
    "hlines(num_sentences[-1],0, len(num_sentences),linestyles='dashed',colors='red')\n",
    "text(0,0.95*num_sentences[-1],num_sentences[-1],fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(20,7))\n",
    "plot(range(NUM_STORIES), num_unique_words)\n",
    "xlabel(\"# Of Stories\")\n",
    "ylabel(\"# Of Unique Words\");\n",
    "title(\"# of Stories vs. # of Unique Words\")\n",
    "grid(axis='both')\n",
    "hlines(num_unique_words[-1],0, len(num_unique_words),linestyles='dashed',colors='red')\n",
    "text(0,0.95*num_unique_words[-1],num_unique_words[-1],fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(20,7))\n",
    "hist(word_counts.values(), bins=250, log=True)\n",
    "title(\"Distribution of Occurances of Individual Words\")\n",
    "xlabel(\"# of Occurances\")\n",
    "ylabel(\"Count of Individual Words\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prepare Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Size of vocabulary to use\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "\n",
    "# Skip n most common words\n",
    "NUM_COMMON_WORDS_TO_SKIP = 0\n",
    "\n",
    "if MAX_VOCAB_SIZE < len(unique_words):\n",
    "    \n",
    "    # Take only the MAX_SIZE most common words\n",
    "    words = list(sorted(word_counts.items(),key=operator.itemgetter(1),reverse=True)) \\\n",
    "                        [NUM_COMMON_WORDS_TO_SKIP:NUM_COMMON_WORDS_TO_SKIP+MAX_VOCAB_SIZE]\n",
    "    unique_words = set(map(lambda x: x[0], words)).union(['<PAD>','<UNK>','<NUM>'])\n",
    "\n",
    "# Lookup tables\n",
    "word_to_index = dict([(word,index) for (index,word) in enumerate(unique_words)])\n",
    "index_to_word = dict([(index,word) for (index,word) in enumerate(unique_words)])\n",
    "\n",
    "\n",
    "unk_index = word_to_index['<UNK>']\n",
    "\n",
    "# Helper functions for translating back and forth between indices and text\n",
    "def sentence_to_indices(sentence):\n",
    "    return [word_to_index.get(x, unk_index) for x in sentence.split(' ') ]\n",
    "\n",
    "def indices_to_sentence(indices):\n",
    "    return ' '.join(map(lambda index: index_to_word.get(index, '<UNK>'), indices))\n",
    "\n",
    "VOCAB_SIZE = len(unique_words)\n",
    "print(\"Vocabulary Size: \" + str(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for CBOW: for each target words, compute context left and right context words and combine them\n",
    "\n",
    "CONTEXT_SIZE = 3\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "sentence_count = 0\n",
    "\n",
    "\n",
    "for this_sentence in data:\n",
    "    \n",
    "    # Pad at beginning and end of sentences \n",
    "    this_sentence = ['<PAD>' for _ in range(CONTEXT_SIZE)] + this_sentence.split(' ') \\\n",
    "        + ['<PAD>' for _ in range(CONTEXT_SIZE)] \n",
    "    \n",
    "    # Convert words to indices in vocabulary\n",
    "    indices = [word_to_index.get(x, unk_index) for x in this_sentence]\n",
    "    \n",
    "    # Prepare context words in X, target word in y\n",
    "    for target_location in range(CONTEXT_SIZE, len(indices) - CONTEXT_SIZE):\n",
    "        target = indices[target_location]\n",
    "        context = np.zeros(CONTEXT_SIZE*2, dtype=np.long)\n",
    "        \n",
    "        left_context = list(enumerate(range(target_location-CONTEXT_SIZE, target_location)))\n",
    "        right_context = list(enumerate(range(target_location+1, target_location+CONTEXT_SIZE+1), CONTEXT_SIZE))\n",
    "        for (index, context_location) in left_context + right_context:\n",
    "            context[index] = indices[context_location]\n",
    "        X.append(context)\n",
    "        y.append(target)\n",
    "        \n",
    "    sentence_count +=1\n",
    "    if sentence_count % 5000 == 0:\n",
    "        print(\"Prepared {} sentences\".format(sentence_count))\n",
    "        \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 100\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CBOW, self).__init__(**kwargs)\n",
    "        self.embeddings = nn.Embedding(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "        self.linear1 = nn.Linear(EMBEDDING_SIZE, HIDDEN_SIZE)\n",
    "        self.linear2 = nn.Linear(HIDDEN_SIZE, VOCAB_SIZE)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x).sum(dim=1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "cbow = CBOW().cuda()\n",
    "optimizer = torch.optim.Adam(cbow.parameters())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_index, (context, target) in enumerate(dataloader):\n",
    "        context, target = context.long().cuda(), target.long().cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        prediction = cbow(context)\n",
    "        loss = F.cross_entropy(prediction, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_index % 10 == 0:\n",
    "            print(\"Epoch: {}   Batch: {}   Loss: {}\".format(epoch, batch_index, loss.item()))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = cbow.embeddings.weight\n",
    "\n",
    "# Given a set of embeddings and a word, find the n words with the closest embeddings\n",
    "def get_closest(embeddings, word, n=10):\n",
    "    src_embeddings = embeddings[word_to_index[word]]\n",
    "    distances = torch.zeros(VOCAB_SIZE)\n",
    "    for i in range(VOCAB_SIZE):\n",
    "        distances[i] = torch.dist(src_embeddings, embeddings[i])\n",
    "    distances[word_to_index[word]]=999999\n",
    "    n_best = torch.argsort(distances)[:n]\n",
    "    return list([index_to_word[i.item()] for i in n_best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_closest(embeddings, 'crime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 40\n",
    "HIDDEN_SIZE = 128\n",
    "\n",
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Skipgram, self).__init__(**kwargs)\n",
    "        self.embeddings = nn.Embedding(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "        self.linear1 = nn.Linear(EMBEDDING_SIZE, HIDDEN_SIZE)\n",
    "        self.outputs = []\n",
    "        \n",
    "        # Multiple outputs - for each context word we want to predict\n",
    "        for i in range(CONTEXT_SIZE * 2):\n",
    "            layer = nn.Linear(HIDDEN_SIZE, VOCAB_SIZE)\n",
    "            setattr(self, 'output' + str(i+1), layer)\n",
    "            self.outputs.append(layer)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        results = []\n",
    "        for i in range(CONTEXT_SIZE*2):\n",
    "            results.append(F.relu(self.outputs[i](x)))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "skipgram = Skipgram().cuda()\n",
    "optimizer = torch.optim.Adam(skipgram.parameters())\n",
    "\n",
    "\n",
    "\n",
    "# We already have context and target words - we now need the single words at the input and the\n",
    "# multiple words at the output\n",
    "dataset = TensorDataset(torch.Tensor(y), torch.Tensor(X))\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_index, (context, target) in enumerate(dataloader):\n",
    "        context, target = context.long().cuda(), target.long().cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = skipgram(context)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = torch.zeros(1).cuda()\n",
    "        for i in range(CONTEXT_SIZE*2):\n",
    "            \n",
    "            # Calculate total loss by summing the individual losses\n",
    "            loss += F.cross_entropy(predictions[i], target[:,i])\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if batch_index % 10 == 0:\n",
    "            print(\"Epoch: {}   Batch: {}   Loss: {}\".format(epoch, batch_index, loss.item()))\n",
    "        \n",
    "         \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = skipgram.embeddings.weight\n",
    "get_closest(embeddings, 'crime')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
