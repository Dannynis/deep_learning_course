{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline  \n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import operator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('nosleep.csv')\n",
    "\n",
    "# Some posts were deleted\n",
    "df = df[df.Author != '[deleted]']\n",
    "\n",
    "# Some text is empty\n",
    "df = df[df.Text.isna() == False]\n",
    "\n",
    "# Some text is too short\n",
    "df = df[df.Text.str.len() > 100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.Flair=='Series') | (df.Flair=='None')]\n",
    "df.Flair.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Classification\n",
    "### 2.1 Preprocess Text and Prepare Word Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Number of stories to take from each category\n",
    "NUM_STORIES = 1000\n",
    "\n",
    "# Number of words to take from end of each story\n",
    "NUM_FINAL_WORDS = 30\n",
    "\n",
    "# Take the first NUM_STORIES stories\n",
    "stories_none = df[df.Flair == 'None'][:NUM_STORIES]\n",
    "stories_series = df[df.Flair == 'Series'][:NUM_STORIES]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_words_from_story(text, num_words = NUM_FINAL_WORDS):\n",
    "    \n",
    "    paragraphs = list(filter(lambda x: len(x) > 0, text.split(\"\\n\")))\n",
    "    entire_text = ''\n",
    "    for this_paragraph in paragraphs:\n",
    "        \n",
    "        # If the paragram markdown contains a link, remove it\n",
    "        if \"[\" in this_paragraph and 'https://' in this_paragraph:\n",
    "            continue\n",
    "            \n",
    "        # Remove special symbols\n",
    "        if this_paragraph.startswith(\"&amp;\"):\n",
    "            continue\n",
    "        \n",
    "        \n",
    "      \n",
    "            \n",
    "        # Remove punctuation symbols\n",
    "        for char in string.punctuation + \"’”“…—-\":\n",
    "            this_paragraph = this_paragraph.replace(char, ' ')\n",
    "                \n",
    "        # Remove multiple whitespace\n",
    "        this_paragraph = re.sub(\"\\s{2,}\", \" \", this_paragraph)\n",
    "\n",
    "        # Remove initial and trailing whitespace, and make everything lowercase\n",
    "        this_paragraph = this_paragraph.strip().lower()\n",
    "\n",
    "        # Replace numbers with a special <NUM> token\n",
    "        this_paragraph = re.sub(\"\\d+\", \"<NUM>\", this_paragraph)\n",
    "\n",
    "        entire_text += this_paragraph + \" \"\n",
    "    \n",
    "    return entire_text.split(' ')[-num_words:]\n",
    "\n",
    "words_none = []\n",
    "words_series = []\n",
    "\n",
    "unique_words = set(['<UNK>','<NUM>'])\n",
    "\n",
    "for this_story in stories_series.Text:\n",
    "    final_words = get_words_from_story(this_story)\n",
    "    if len(final_words) < NUM_FINAL_WORDS:\n",
    "        continue\n",
    "    words_series.append(final_words)\n",
    "    unique_words = unique_words.union(set(final_words))\n",
    "    \n",
    "for this_story in stories_none.Text:\n",
    "    final_words = get_words_from_story(this_story)\n",
    "    if len(final_words) < NUM_FINAL_WORDS:\n",
    "        continue\n",
    "    words_none.append(final_words)\n",
    "    unique_words = unique_words.union(set(final_words))\n",
    "    \n",
    "\n",
    "\n",
    "print(\"Vocabulary Size: {}\".format(len(unique_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_choices = choice(range(NUM_STORIES), 5)\n",
    "\n",
    "for i in random_choices:\n",
    "    print(words_none[i])\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "for i in random_choices:\n",
    "    print(words_series[i])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Prepare Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lookup tables\n",
    "word_to_index = dict([(word,index) for (index,word) in enumerate(unique_words)])\n",
    "index_to_word = dict([(index,word) for (index,word) in enumerate(unique_words)])\n",
    "\n",
    "\n",
    "unk_index = word_to_index['<UNK>']\n",
    "\n",
    "# Helper functions for translating back and forth between indices and text\n",
    "def sentence_to_indices(sentence):\n",
    "    return [word_to_index.get(x, unk_index) for x in sentence.split(' ') ]\n",
    "\n",
    "def indices_to_sentence(indices):\n",
    "    return ' '.join(map(lambda index: index_to_word.get(index, '<UNK>'), indices))\n",
    "\n",
    "VOCAB_SIZE = len(unique_words)\n",
    "print(\"Vocabulary Size: \" + str(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_none = np.zeros((len(words_none), NUM_FINAL_WORDS))\n",
    "indices_series = np.zeros((len(words_series), NUM_FINAL_WORDS))\n",
    "\n",
    "\n",
    "for i in range(indices_none.shape[0]):\n",
    "    for j in range(NUM_FINAL_WORDS):\n",
    "        indices_none[i,j] = word_to_index[words_none[i][j]]\n",
    "        \n",
    "for i in range(indices_series.shape[0]):\n",
    "    for j in range(NUM_FINAL_WORDS):\n",
    "        indices_series[i,j] = word_to_index[words_series[i][j]]\n",
    "        \n",
    "print(\"Indices_none shape: {}\".format(indices_none.shape))\n",
    "print(\"Indices_series shape: {}\".format(indices_series.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((indices_none, indices_series))\n",
    "\n",
    "y = np.concatenate((np.repeat(0,indices_none.shape[0]), np.repeat(1,indices_series.shape[0])))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 70\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Model, self).__init__(**kwargs)\n",
    "        self.embeddings = nn.Embedding(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "        self.lstm = nn.LSTM(input_size=EMBEDDING_SIZE, hidden_size=HIDDEN_SIZE, bidirectional=True,\n",
    "                           batch_first=True)\n",
    "        self.fc1 = nn.Linear(NUM_FINAL_WORDS, NUM_FINAL_WORDS)\n",
    "        self.output = nn.Linear(NUM_FINAL_WORDS, 2)\n",
    "        \n",
    "        self.attn = nn.Linear(2*HIDDEN_SIZE, 2*HIDDEN_SIZE)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        embeddings = self.embeddings(x) # BATCH_SIZE * NUM_FINAL_WORDS * EMBEDDING_SIZE\n",
    "       \n",
    "        \n",
    "        # Only use the (aggregated) hidden state at time t\n",
    "        x,_ = self.lstm(embeddings)    # BATCH_SIZE * NUM_FINAL_WORDS * (2 * HIDDEN_SIZE)\n",
    "        \n",
    "        # Attention implementation\n",
    "        weights = F.softmax(self.attn(x), dim=1)\n",
    "        x = torch.bmm(weights, x.transpose(1,2))\n",
    "        x = x.sum(dim=2)\n",
    "        \n",
    "    \n",
    "        # Run through fully-connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.output(x))\n",
    "        \n",
    "        return x.squeeze(0), weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model = Model().cuda()\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_dataset = TensorDataset(torch.Tensor(X_train).long(), torch.Tensor(y_train).long())\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_index, (sequence, target) in enumerate(train_dataloader):\n",
    "        \n",
    "        sequence, target = sequence.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        predictions,_ = model(sequence)\n",
    "        loss = F.cross_entropy(predictions, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_index % 5 == 0:\n",
    "            print(\"Epoch: {}   Batch: {}   Loss: {}\".format(epoch, batch_index, loss.item()))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_results,_ = model(torch.Tensor(X_test).long().cuda())\n",
    "test_results = test_results.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "print(classification_report(y_test, test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_index = choice(range(len(X_test)))\n",
    "\n",
    "_, weights = model(torch.Tensor(X_test[sequence_index:sequence_index+1]).long().cuda())\n",
    "\n",
    "weights = F.softmax(weights.sum(dim=-1)[0])\n",
    "\n",
    "words = list([\"'\" + index_to_word[j] + \"'\" for j in X_test[sequence_index]])\n",
    "\n",
    "figure(figsize=(20,7))\n",
    "bar(words, weights.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
